# _AutoJudge: Predicting Programming Problem Difficulty_

# Project Overview

Online coding platforms such as Codeforces, CodeChef, and Kattis categorize programming problems into difficulty levels (Easy / Medium / Hard) and assign numerical difficulty scores. These classifications are often subjective and based on human judgment and user feedback.

AutoJudge is an end-to-end Machine Learning system that automatically predicts:

_Difficulty Class: Easy / Medium / Hard (Classification)_

_Difficulty Score: A numerical difficulty value (Regression)_

The predictions are made using only the textual information of a programming problem, including its description, input format, and output format.
The project also includes a local web interface that allows users to paste a new problem description and instantly receive predictions.

# Dataset Used

A dataset of 4,112 programming problems was used. Each problem includes:

1.title

2.description

3.input_description

4.output_description

5.problem_class (Easy / Medium / Hard)

6.problem_score (numerical difficulty score)

The dataset was sourced via web scraping from competitive programming platforms and was provided with pre-labeled difficulty classes and scores. No manual labeling was performed.

# Approach and Methodology
_1Ô∏è. Data Preprocessing_

‚úî Combined all textual fields (description, input_description, output_description) into a single text corpus

‚úî Removed HTML tags, punctuation, and special characters

‚úî Converted text to lowercase

‚úî Handled missing values

‚úî Created additional handcrafted features such as: Text length, Number of mathematical symbols and Frequency of algorithm-related keywords (dp, graph, tree, recursion, binary search)

_2. Feature Extraction_

‚úî TF-IDF Vectorization

    n-grams: (1, 3)

    Maximum features: 8000

‚úî Numeric features were scaled using StandardScaler

‚úî Final feature matrix created by concatenating TF-IDF vectors with numeric features

_3Ô∏è. Classification Models_

The following classification models were evaluated:

‚úî Logistic Regression (balanced class weights)

‚úî Linear Support Vector Machine (SVM)

‚úî Multinomial Naive Bayes (TF-IDF only, baseline)

_Final choice: Linear SVM, due to the best balance between precision, recall, and macro-F1 score._

‚úî A hierarchical classification strategy was also explored:

_Stage 1: Hard vs Not-Hard_

_Stage 2: Easy vs Medium_-

This analysis highlighted strong signals for detecting Hard problems while exposing overlap between Easy and Medium classes.

_4Ô∏è. Regression Model_

‚úî Random Forest Regressor

    Used to predict a continuous difficulty score

    Chosen for robustness and ability to model non-linear relationships

_The regression model is independent of the classification model and provides a smoother difficulty estimate._

# üìà Evaluation Metrics
_Classification Performance (Linear SVM)_

‚úî Accuracy: ~50%

‚úî Macro F1-score: ~0.49

‚úî Hard class recall: ~0.61

_Note: Difficulty classification is inherently subjective, and significant overlap exists between Easy and Medium problems. Results reflect the realistic performance ceiling for text-only classification._

_Regression Performance (Random Forest)_

‚úî Mean Absolute Error (MAE): ~1.541

‚úî Root Mean Squared Error (RMSE): ~1.94

Residual analysis shows no systematic bias and stable predictions across difficulty classes.

# üåê Web Interface

A Streamlit-based web application is provided that allows users to:

Paste:

Problem Description

Input Description

Output Description

Click Predict Difficulty

View:

‚úî Predicted Difficulty Class (Easy / Medium / Hard)

‚úî Predicted Difficulty Score (numerical)

The application runs locally and loads pre-trained models without retraining.



